{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mesoscopic V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### command to show execution time\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "### retrieve gutenberg book \n",
    "\n",
    "from gutenberg.query import get_metadata\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "def retrieve_text(index):\n",
    "    try:\n",
    "        text = strip_headers(load_etext(index)).strip()\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "def retrieve_author(index):\n",
    "    return list(get_metadata('author', index))[0]\n",
    "\n",
    "def retrieve_title(index):\n",
    "    return list(get_metadata('title', index))\n",
    "\n",
    "def retrieve_languages(index):\n",
    "    return list(get_metadata('language', index))\n",
    "\n",
    "def retrieve_subjects(index):\n",
    "    return list(get_metadata('subject', index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### text standardization\n",
    "\n",
    "def standardize_text(book, cutting_flag=False):\n",
    "  \n",
    "    if cutting_flag:\n",
    "        marker = 'chapter i.'\n",
    "        flag = False\n",
    "        for i in range(len(book)):\n",
    "            if book[i:i+len(marker)].lower() == marker:\n",
    "                if flag:\n",
    "                    book = book[i:]\n",
    "                    break\n",
    "                else:\n",
    "                    flag = True\n",
    "\n",
    "    # removing all \\r\n",
    "    book = book.replace('\\r', '')\n",
    "    # marking all paragraph starts (\\n\\n) with .\\r for later \n",
    "    book = book.replace('\\n\\n', '.\\r')\n",
    "    # removing all \\n since they are not marking any paragraphs\n",
    "    book = book.replace('\\n', ' ')\n",
    "    # replacing all \\r with \\n\\n to remark the paragraphs \n",
    "    book = book.replace('\\r', '\\n\\n')\n",
    "    # replacing any possible duplicated full stops\n",
    "    book = book.replace('..', '.')\n",
    "    # reconstructing possible damaged ellipsis (...)\n",
    "    book = book.replace('..', '...')\n",
    "    # removing underscores\n",
    "    book = book.replace('_', '')\n",
    "  \n",
    "    \n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### remove chapter markers\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_chapter_markers(book):\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    paragraphs_new = []\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split(' ')\n",
    "        if len(words) == 1 and words[0] in ['introduction', 'book', 'preface']:\n",
    "            continue\n",
    "        if re.match('^\\w+ [IVXLCDM\\d]+[\\.]*', paragraph.upper().strip()) and len(words) <= 10:\n",
    "            continue\n",
    "        if not re.match('.*[\\w]+.*', paragraph):\n",
    "            continue\n",
    "        paragraphs_new.append(paragraph)\n",
    "    return '\\n\\n'.join(paragraphs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "### solving anaphors for whole text\n",
    "### better than for each paragraph, we're able to also get references from different paragraphs\n",
    "\n",
    "import neuralcoref \n",
    "import spacy\n",
    "\n",
    "def solve_anaphors(book, nlp):\n",
    "    doc = nlp(book)\n",
    "\n",
    "    return doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "### stopwords removal\n",
    "\n",
    "import nltk \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_list = text.split(' ')\n",
    "    stop_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text_list if word not in stop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### apply syntatic parsin to text to get:\n",
    "###  whole text, only having removed the stopwords\n",
    "###  (actor, action): root.dep_ = nsubj (actor), root.head.text = action\n",
    "###  (action, object): root.dep_ = dobj (object), root.head.text = action\n",
    "### in the 2 latter cases, we apply lemmatization after getting the pairs\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_syntatic_pairs(node_text, nlp, lemmatizer):    \n",
    "    doc = nlp(node_text)\n",
    "    new_text = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk.root.dep_ in ['dobj', 'nsubj']):\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.text, pos=wordnet.NOUN) + '_' + lemmatizer.lemmatize(chunk.root.head.text, pos=wordnet.VERB))\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.text, pos=wordnet.NOUN))\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.head.text, pos=wordnet.VERB))\n",
    "\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "def transform_text_to_syntatic_pairs(book):        \n",
    "    book = book.lower()\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    neuralcoref.add_to_pipe(nlp)\n",
    "    pairs = []\n",
    "    for paragraph in paragraphs:\n",
    "        pair = get_syntatic_pairs(solve_anaphors(paragraph, nlp), nlp, lemmatizer)\n",
    "        # WRONG: if len(pair) > 0:\n",
    "        # i have to append pair even if it is empty otherwise the window will consider nodes that are \n",
    "        # actually more further apart than wanted\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting network node text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### joining paragraphs to reach min size of the text for a single node\n",
    "\n",
    "def get_node_texts(pairs_per_paragraph, window_size):\n",
    "    dic = {}\n",
    "\n",
    "    for id, paragraph in enumerate(pairs_per_paragraph):\n",
    "        dic[id] = '\\n\\n'.join(pairs_per_paragraph[max(0,id-window_size):min(len(pairs_per_paragraph), id+window_size+1)])\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#### we already have the nodes, for the edges we'll use TF_IDF and cosine similarity to define which nodes to connect\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b): \n",
    "    if norm(a) * norm(b) == 0:\n",
    "        return 0\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def get_sim_scores(dic):    \n",
    "    # now i can filter by length, cause the windows are done and i will only eliminate nodes in which all of the considered\n",
    "    # paragraphs have no tuples at all. \n",
    "    # to avoid causing different window size networks to have a different amount of nodes, I'll just strip the value of dic\n",
    "    # making it a empty doc, but making sure it at least exists and also that that are no empty tuples considered when \n",
    "    # running tf idf\n",
    "    docs = [dic[key].strip() for key in sorted(dic.keys()) if len(dic[key]) > 0]\n",
    "\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(docs)\n",
    "\n",
    "    tfidf_of_docs = []\n",
    "    for row in tfIdf:\n",
    "        tfidf_of_docs.append(np.squeeze(np.asarray(row.todense())))\n",
    "\n",
    "    scores = np.zeros((len(docs), len(docs)))\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(i+1, len(docs)):\n",
    "            scores[i][j] = cos_sim(tfidf_of_docs[i], tfidf_of_docs[j])\n",
    "\n",
    "    return scores, len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "### creating graph using igraph\n",
    "\n",
    "from igraph import *\n",
    "import numpy as np \n",
    "\n",
    "def create_graph(scores, N, window_size):\n",
    "    edges = []\n",
    "    types = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            # no overlap\n",
    "            if j - i > 2*window_size: \n",
    "                if scores[i][j] > 0:\n",
    "                    edges.append((i, j))\n",
    "                    types.append('similarity')\n",
    "                    weights.append(scores[i][j])\n",
    "            elif j == i + 1:\n",
    "                edges.append((i, j))\n",
    "                types.append('sequence')\n",
    "                weights.append(2)\n",
    "    \n",
    "    G = Graph(directed=False)\n",
    "    G.add_vertices([i for i in range(N)])\n",
    "    G.add_edges(edges)\n",
    "    G.es['weight'] = weights\n",
    "    G.es['type'] = types\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def run(book_id):\n",
    "    book = standardize_text(retrieve_text(book_id))\n",
    "#     print(len(book))\n",
    "    if len(book) == 0 : \n",
    "        print('could not retrieve', book_id)\n",
    "        return\n",
    "    if len(book) >= 1000000: \n",
    "        print('book is too big')\n",
    "        return\n",
    "\n",
    "    book = remove_chapter_markers(book)\n",
    "\n",
    "    pairs_per_paragraph = transform_text_to_syntatic_pairs(book)\n",
    "    \n",
    "    f = open('./pca 300 networks new right/network_' + str(book_id) + '_processed_text.txt', 'w', encoding='utf-8')\n",
    "    f.write('\\n\\n'.join(pairs_per_paragraph))\n",
    "    f.close()\n",
    "\n",
    "    window = 1\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "        \n",
    "    window = 4\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "        \n",
    "    window = 9\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating running the algorithm for all 300 books selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28284\n",
      "28675\n",
      "29291\n",
      "29406\n",
      "29642\n",
      "29764\n",
      "29860\n",
      "3047\n",
      "3056\n",
      "30576\n",
      "30851\n",
      "30865\n",
      "30873\n",
      "31029\n",
      "31387\n",
      "31672\n",
      "31891\n",
      "32090\n",
      "32120\n",
      "32129\n",
      "32185\n",
      "32250\n",
      "322\n",
      "32708\n",
      "32931\n",
      "32997\n",
      "33004\n",
      "33113\n",
      "33195\n",
      "33221\n",
      "33233\n",
      "33416\n",
      "33437\n",
      "33976\n",
      "34020\n",
      "34395\n",
      "34488\n",
      "3472\n",
      "34832\n",
      "35975\n",
      "36531\n",
      "36540\n",
      "36585\n",
      "36717\n",
      "36728\n",
      "36804\n",
      "36828\n",
      "36858\n",
      "3695\n",
      "37263\n",
      "37431\n",
      "37544\n",
      "37681\n",
      "37909\n",
      "38679\n",
      "38952\n",
      "38960\n",
      "39211\n",
      "3926\n",
      "39300\n",
      "39372\n",
      "39443\n",
      "39595\n",
      "39629\n",
      "39747\n",
      "39933\n",
      "39957\n",
      "40255\n",
      "40340\n",
      "40372\n",
      "40525\n",
      "40793\n",
      "40966\n",
      "41218\n",
      "41267\n",
      "41408\n",
      "41524\n",
      "41632\n",
      "41655\n",
      "41703\n",
      "41777\n",
      "41893\n",
      "41896\n",
      "42155\n",
      "42320\n",
      "42417\n",
      "42426\n",
      "4262\n",
      "42630\n",
      "42772\n",
      "42860\n",
      "42990\n",
      "43218\n",
      "43455\n",
      "43731\n",
      "43937\n",
      "44428\n",
      "44780\n",
      "44862\n",
      "45053\n",
      "45214\n",
      "45395\n",
      "45452\n",
      "45728\n",
      "45926\n",
      "45929\n",
      "45989\n",
      "46006\n",
      "46276\n",
      "46315\n",
      "46320\n",
      "46338\n",
      "46403\n",
      "46619\n",
      "46794\n",
      "46889\n",
      "47242\n",
      "47585\n",
      "47731\n",
      "47762\n",
      "47923\n",
      "4798\n",
      "48184\n",
      "48382\n",
      "48702\n",
      "48865\n",
      "48953\n",
      "48996\n",
      "49324\n",
      "49391\n",
      "49490\n",
      "49615\n",
      "4965\n",
      "49718\n",
      "49794\n",
      "4987\n",
      "51022\n",
      "51285\n",
      "5145\n",
      "51734\n",
      "51913\n",
      "51922\n",
      "52378\n",
      "52609\n",
      "53945\n",
      "54496\n",
      "54743\n",
      "54990\n",
      "55002\n",
      "55212\n",
      "55376\n",
      "55557\n",
      "55658\n",
      "55894\n",
      "56006\n",
      "56040\n",
      "56321\n",
      "56347\n",
      "56675\n",
      "57010\n",
      "57308\n",
      "57512\n",
      "57680\n",
      "57798\n",
      "57989\n",
      "58013\n",
      "5805\n",
      "58677\n",
      "58718\n",
      "58863\n",
      "59354\n",
      "59428\n",
      "5952\n",
      "59648\n",
      "60010\n",
      "60021\n",
      "60277\n",
      "60318\n",
      "60527\n",
      "60894\n",
      "60904\n",
      "61247\n",
      "61348\n",
      "6179\n",
      "62151\n",
      "63039\n",
      "63642\n",
      "6808\n",
      "7060\n",
      "7126\n",
      "7253\n",
      "7504\n",
      "7885\n",
      "7952\n",
      "8327\n",
      "8642\n",
      "9395\n",
      "9440\n",
      "9808\n",
      "9929\n",
      "9937\n",
      "time: 18h 28min 44s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_old = '9_nsubj'\n",
    "path_old = './pca 300 networks/' + method_old + '/'\n",
    "path_new = './pca 300 networks new right/'\n",
    "directory = os.fsencode(path_old)\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = path_old + os.fsdecode(file)\n",
    "    if filename.endswith(\".p\"):\n",
    "        start = len(path_old) + len('network_')\n",
    "        end = len('_' + method_old + '.p')\n",
    "        gutenberg_id = int(filename[start:-end])\n",
    "        \n",
    "        if os.path.exists(path_new + 'network_' + str(gutenberg_id) + '_w9.p'):\n",
    "            continue\n",
    "        \n",
    "        print(gutenberg_id)\n",
    "        run(gutenberg_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
