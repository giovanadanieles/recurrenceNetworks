{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating recurrence networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### command to show execution time\n",
    "\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gutenberg project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.86 s\n"
     ]
    }
   ],
   "source": [
    "### retrieve gutenberg book \n",
    "\n",
    "from gutenberg.query import get_metadata\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "def retrieve_text(index):\n",
    "    try:\n",
    "        text = strip_headers(load_etext(index)).strip()\n",
    "    except:\n",
    "        text = \"\"\n",
    "    return text\n",
    "\n",
    "def retrieve_author(index):\n",
    "    return list(get_metadata('author', index))[0]\n",
    "\n",
    "def retrieve_title(index):\n",
    "    return list(get_metadata('title', index))\n",
    "\n",
    "def retrieve_languages(index):\n",
    "    return list(get_metadata('language', index))\n",
    "\n",
    "def retrieve_subjects(index):\n",
    "    return list(get_metadata('subject', index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### text standardization\n",
    "\n",
    "def standardize_text(book, cutting_flag=False):\n",
    "  \n",
    "    if cutting_flag:\n",
    "        marker = 'chapter i.'\n",
    "        flag = False\n",
    "        for i in range(len(book)):\n",
    "            if book[i:i+len(marker)].lower() == marker:\n",
    "                if flag:\n",
    "                    book = book[i:]\n",
    "                    break\n",
    "                else:\n",
    "                    flag = True\n",
    "\n",
    "    # removing all \\r\n",
    "    book = book.replace('\\r', '')\n",
    "    # marking all paragraph starts (\\n\\n) with .\\r for later \n",
    "    book = book.replace('\\n\\n', '.\\r')\n",
    "    # removing all \\n since they are not marking any paragraphs\n",
    "    book = book.replace('\\n', ' ')\n",
    "    # replacing all \\r with \\n\\n to remark the paragraphs \n",
    "    book = book.replace('\\r', '\\n\\n')\n",
    "    # replacing any possible duplicated full stops\n",
    "    book = book.replace('..', '.')\n",
    "    # reconstructing possible damaged ellipsis (...)\n",
    "    book = book.replace('..', '...')\n",
    "    # removing underscores\n",
    "    book = book.replace('_', '')\n",
    "   \n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### remove chapter markers\n",
    "\n",
    "import re\n",
    "\n",
    "def remove_chapter_markers(book):\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    paragraphs_new = []\n",
    "    for paragraph in paragraphs:\n",
    "        words = paragraph.split(' ')\n",
    "        if len(words) == 1 and words[0] in ['introduction', 'book', 'preface']:\n",
    "            continue\n",
    "        if re.match('^\\w+ [IVXLCDM\\d]+[\\.]*', paragraph.upper().strip()) and len(words) <= 10:\n",
    "            continue\n",
    "        if not re.match('.*[\\w]+.*', paragraph):\n",
    "            continue\n",
    "        paragraphs_new.append(paragraph)\n",
    "    return '\\n\\n'.join(paragraphs_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.08 s\n"
     ]
    }
   ],
   "source": [
    "### solving anaphors for whole text\n",
    "### better than for each paragraph, we're able to also get references from different paragraphs\n",
    "\n",
    "import neuralcoref \n",
    "import spacy\n",
    "\n",
    "def solve_anaphors(book, nlp):\n",
    "    doc = nlp(book)\n",
    "\n",
    "    return doc._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.53 s\n"
     ]
    }
   ],
   "source": [
    "### stopwords removal\n",
    "\n",
    "import nltk \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_list = text.split(' ')\n",
    "    stop_list = stopwords.words('english')\n",
    "    return ' '.join([word for word in text_list if word not in stop_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syntatic parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### apply syntatic parsin to text to get:\n",
    "###  whole text, only having removed the stopwords\n",
    "###  (actor, action): root.dep_ = nsubj (actor), root.head.text = action\n",
    "###  (action, object): root.dep_ = dobj (object), root.head.text = action\n",
    "### in the 2 latter cases, we apply lemmatization after getting the pairs\n",
    "\n",
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_syntatic_pairs(node_text, nlp, lemmatizer):    \n",
    "    doc = nlp(node_text)\n",
    "    new_text = set()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk.root.dep_ in ['dobj', 'nsubj']):\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.text, pos=wordnet.NOUN) + '_' + lemmatizer.lemmatize(chunk.root.head.text, pos=wordnet.VERB))\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.text, pos=wordnet.NOUN))\n",
    "            new_text.add(lemmatizer.lemmatize(chunk.root.head.text, pos=wordnet.VERB))\n",
    "\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "def transform_text_to_syntatic_pairs(book):        \n",
    "    book = book.lower()\n",
    "    paragraphs = book.split('\\n\\n')\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    neuralcoref.add_to_pipe(nlp)\n",
    "    pairs = []\n",
    "    for paragraph in paragraphs:\n",
    "        pair = get_syntatic_pairs(solve_anaphors(paragraph, nlp), nlp, lemmatizer)\n",
    "        # WRONG: if len(pair) > 0:\n",
    "        # i have to append pair even if it is empty otherwise the window will consider nodes that are \n",
    "        # actually more further apart than wanted\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting network node text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "### joining paragraphs to reach min size of the text for a single node\n",
    "\n",
    "def get_node_texts(pairs_per_paragraph, window_size):\n",
    "    dic = {}\n",
    "\n",
    "    for id, paragraph in enumerate(pairs_per_paragraph):\n",
    "        dic[id] = '\\n\\n'.join(pairs_per_paragraph[max(0,id-window_size):min(len(pairs_per_paragraph), id+window_size+1)])\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "#### we already have the nodes, for the edges we'll use TF_IDF and cosine similarity to define which nodes to connect\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b): \n",
    "    if norm(a) * norm(b) == 0:\n",
    "        return 0\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def get_sim_scores(dic):    \n",
    "    # now i can filter by length, cause the windows are done and i will only eliminate nodes in which all of the considered\n",
    "    # paragraphs have no tuples at all. \n",
    "    # to avoid causing different window size networks to have a different amount of nodes, I'll just strip the value of dic\n",
    "    # making it a empty doc, but making sure it at least exists and also that that are no empty tuples considered when \n",
    "    # running tf idf\n",
    "    docs = [dic[key].strip() for key in sorted(dic.keys()) if len(dic[key]) > 0]\n",
    "\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(docs)\n",
    "\n",
    "    tfidf_of_docs = []\n",
    "    for row in tfIdf:\n",
    "        tfidf_of_docs.append(np.squeeze(np.asarray(row.todense())))\n",
    "\n",
    "    scores = np.zeros((len(docs), len(docs)))\n",
    "\n",
    "    for i in range(len(docs)):\n",
    "        for j in range(i+1, len(docs)):\n",
    "            scores[i][j] = cos_sim(tfidf_of_docs[i], tfidf_of_docs[j])\n",
    "\n",
    "    return scores, len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 63 ms\n"
     ]
    }
   ],
   "source": [
    "### creating graph using igraph\n",
    "\n",
    "from igraph import *\n",
    "import numpy as np \n",
    "\n",
    "def create_graph(scores, N, window_size):\n",
    "    edges = []\n",
    "    types = []\n",
    "    weights = []\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            # no overlap\n",
    "            if j - i > 2*window_size: \n",
    "                if scores[i][j] > 0:\n",
    "                    edges.append((i, j))\n",
    "                    types.append('similarity')\n",
    "                    weights.append(scores[i][j])\n",
    "            elif j == i + 1:\n",
    "                edges.append((i, j))\n",
    "                types.append('sequence')\n",
    "                weights.append(2)\n",
    "    \n",
    "    G = Graph(directed=False)\n",
    "    G.add_vertices([i for i in range(N)])\n",
    "    G.add_edges(edges)\n",
    "    G.es['weight'] = weights\n",
    "    G.es['type'] = types\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "def run(book_id):\n",
    "    book = standardize_text(retrieve_text(book_id))\n",
    "#     print(len(book))\n",
    "    if len(book) == 0 : \n",
    "        print('could not retrieve', book_id)\n",
    "        return\n",
    "    if len(book) >= 1000000: \n",
    "        print('book is too big')\n",
    "        return\n",
    "\n",
    "    book = remove_chapter_markers(book)\n",
    "\n",
    "    pairs_per_paragraph = transform_text_to_syntatic_pairs(book)\n",
    "    \n",
    "    f = open('./pca 300 networks new right/network_' + str(book_id) + '_processed_text.txt', 'w', encoding='utf-8')\n",
    "    f.write('\\n\\n'.join(pairs_per_paragraph))\n",
    "    f.close()\n",
    "\n",
    "    window = 1\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "        \n",
    "    window = 4\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "        \n",
    "    window = 9\n",
    "    node_dic = get_node_texts(pairs_per_paragraph, window)\n",
    "    scores, N = get_sim_scores(node_dic)\n",
    "    G = create_graph(scores, N, window)\n",
    "    G = filterNetworkEdges([G], 20)[0]\n",
    "    if G.vcount() >= 100: \n",
    "        pickle.dump(G, open('./pca 300 networks new right/network_'+str(book_id)+'_w'+str(window)+'.p', 'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automating running the algorithm for all 300 books selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "method_old = '9_nsubj'\n",
    "path_old = './pca 300 networks/' + method_old + '/'\n",
    "path_new = './pca 300 networks new right/'\n",
    "directory = os.fsencode(path_old)\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "    filename = path_old + os.fsdecode(file)\n",
    "    if filename.endswith(\".p\"):\n",
    "        start = len(path_old) + len('network_')\n",
    "        end = len('_' + method_old + '.p')\n",
    "        gutenberg_id = int(filename[start:-end])\n",
    "        \n",
    "        if os.path.exists(path_new + 'network_' + str(gutenberg_id) + '_w9.p'):\n",
    "            continue\n",
    "        \n",
    "        print(gutenberg_id)\n",
    "        run(gutenberg_id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
